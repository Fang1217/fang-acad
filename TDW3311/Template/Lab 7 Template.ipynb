{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07 - Data Wrangling Skills Part II\n",
    "Two main concepts are covered in this lab: \n",
    "1. data enrichment; and\n",
    "2. feature engineering. \n",
    "Some of the skills that you have learned are needed to complete the exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a.0 Data Enrichment\n",
    " In this section, we will use two data sets. One data set is a fake data and another one is the air quality data from UCI (the data set which has be used in lab06 under section Date Time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tCreate a fake data with movies title and two users’ profile. The rating with value `1` indicates extremely dislike whereas `10` indicates extremely like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# create data. In real world, this data may be from database\n",
    "movies = {'movie_id': [1000, 1001, 1002, 1003],\n",
    "          'title': ['Toy Story', 'Jumanji', 'Iron Man', 'The Secrets of Dumbledore'],\n",
    "          'genre': ['Adventure|Animation|Children', 'Adventure|Children|Fantasy', 'Action|Adventure|Sci-Fi', 'Adventure|Fantasy|Family']}\n",
    "\n",
    "users = {'user_id': [100, 100, 101, 101, 102, 102, 103,103, 103, 104, 104, 105, \n",
    "                     106, 106, 106, 107, 107, 108, 108, 109],\n",
    "         'DOB': [20000221, 20000221, 19901212, 19901212, 19831125, 19831125, \n",
    "                 19880920, 19880920, 19880920, 19750614,19750614, 19600522, 19950505, \n",
    "                 19950505, 19950505, 19770723, 19770723, 20010101, 20010101, 19990608],\n",
    "         'movie_id': [1000, 1001, 1000, 1002, 1001,1002, 1000, 1002, 1002, 1001, 1002, \n",
    "                      1001, 1000, 1001, 1002, 1001, 1002, 1000, 1001, 1002],\n",
    "         'timestamp': [20210728, 20210727, 20210726, 20210728, 20210727, 20210726, 20210728, \n",
    "                       20210727, 20210726,20210728, 20210727, 20210726, 20210728, 20210727, 20210726,\n",
    "                       20210728, 20210727, 20210726, 20210727, 20210726],\n",
    "         'rating': [9,8,7,5,8,7,9,10,5,6,8,8, 9, 8, 6, 8, 7, 8,6,8]}\n",
    "\n",
    "users1 = {'user_id': [110, 110, 111, 112, 112, 112, 113,113, 113, 114, 114, 115],\n",
    "         'DOB': [20020202, 20020202, 19901212, 19831130, 19831130, 19831130, 19880905, \n",
    "                 19880905, 19880905, 19750610,19750610, 19600512],\n",
    "         'movie_id': [1000, 1001, 1000, 1002, 1001,1002, 1000, 1002, 1002, 1001, 1002, 1001],\n",
    "         'timestamp': [20210728, 20210727, 20210726, 20210728, 20210727, \n",
    "                       20210726, 20210728, 20210727, 20210726,20210728, 20210727, 20210726],\n",
    "         'rating': [8,8,9,6,5,8,9,10,8,7,5,8]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Data Frame and Histogram\n",
    "\n",
    "This exercise is to let you practice your python skill in creating data frame and observe the data through histogram.\\\n",
    "Perform the following:\n",
    "\n",
    "Ex 1.1 : Now, save both movies, users and users1 into three different data frames named as `Data1`, `Data2` and `Data3` respectively.\\\n",
    "\\\n",
    "Ex 1.2 : Display the first few rows of each data frame. \\\n",
    "\\\n",
    "Ex 1.3 : Explore the feature “rating” by using histogram and set the number of bins = 5. [Hints: the function is ``.hist(bins=X)``]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tThe genre in Data1 is a string, such as “`Advanture|Animation|Children`”. Change each of the word into a list by manipulating the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the genre column strings at '|' to make lists\n",
    "Data1.genre = Data1.genre.str.split('|')\n",
    "display(Data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tTransform the DOB in Data2 and Data3 from string to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data2['DOB']=Data2['DOB'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))\n",
    "Data3['DOB']=Data3['DOB'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Adding New Columns\n",
    "\n",
    "Ex2.1.\tBased on the DOB, count the age of each user in Data2 and Data3. [Hints: to_datetime(‘today’) will get you the date of today but you need only the year]\\\n",
    "\\\n",
    "Ex2.2.\tAdd the information in Ex2.1 as a new column to Data2 and Data3 respectively. Then, display first few rows of both data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8a.1 Simple Binning\n",
    "\n",
    "1.\tObtain `minAge` and `maxAge` from `Data2` through column `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "minAge = Data2['Age'].min()\n",
    "maxAge = Data2['Age'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set the linspace (linearly spaced values) between `minAge` and `maxAge` to 4 intervals. With 4 intervals, we can set 3 categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=np.linspace(minAge,maxAge,4)\n",
    "labels=['young','middle age','senior']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Use a function called `cut` to have equal width binning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data2['bins'] =pd.cut(Data2['Age'], bins=bins, labels=labels, include_lowest=True)\n",
    "Data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tUse `qcut` and observe the equal frequency binning (approximate). Discuss the differences of output after apply `cut` and `qcut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data2['bin_qcut'] = pd.qcut(Data2['Age'], q=3, labels=labels, precision=1)\n",
    "display(Data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a.2 Clustering and Binning\n",
    "Clustering techniques can be used to split the data and are one of the binning techniques. \n",
    "\n",
    "#### A. In this section, we use k-mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import k-mean from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a variable `Data4` to store the `Age` column information in `Data2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Data4 = Data2[['Age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tSet the number of clusters in k-mean to 3 and train on `Data4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(Data4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tAdd the cluster result from k-mean as an additional columns and name the column as “cluster”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data4.loc[:, 'cluster'] =kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tName the labels by setting 0 as Young, 1 as Middle Age and 2 as Senior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: \"Young\", 1: \"Middle Age\", 2: \"Senior\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tApply the labels to `Data4['cluster']` and add it into Data2 as \"`class_kmeans`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data2[\"class_kmeans\"] = Data4[\"cluster\"].apply(lambda x: labels[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tCompare all the other binning results and observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.\tAnother simple clustering technique is Fisher-Jenks algorithm (natural break). \n",
    "The example is demonstrated here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tInstall jenkspy if you have not done so (Batch file, or executed in windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install jenkspy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tImport the module and set the break of classes to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jenkspy\n",
    "Breaks_nature = jenkspy.jenks_breaks(Data2['Age'], nb_class=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tUse function `cut` to break the data according to the break information in `Breaks_nature` and store the information in a new column `bincut_break`. Show the data of this new column by using histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data2['bincut_break'] = pd.cut(Data2['Age'], bins=Breaks_nature, labels=labels, include_lowest=True)\n",
    "Data2['bincut_break'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tDisplay `Data2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a.3 Concatenate, Merging and Joining\n",
    "\n",
    "We can join data frames together to check the intersection/overlapping records between two data sets or combine related information together into one data frame and form a larger data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tApply a simple concatenate function to combine Data1 and Data2 by columns. Print screen your result and paste in a word document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1 = pd.concat([Data1, Data2], axis=1)\n",
    "display(con1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tAdd in the parameter join=’inner’. Observe the difference between the output with join and the output in No. 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1 = pd.concat([Data1, Data2], axis=1, join='inner')\n",
    "display(con1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tThe following command is applying merge to combine Data1 and Data2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_Merge = pd.merge(Data1, Data2, how=\"outer\",on='movie_id', suffixes=(\"_x\",\"_y\"))\n",
    "display(outer_Merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tUse join. Print screen the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tObserve and dicuss the differences in the result if any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Combining the Data\n",
    "Ex3.1.\tConcatenate Data1 and Data2 by rows.\n",
    "\n",
    "Ex3.2.\tConcatenate Data2 and Data3 by columns.\n",
    "\n",
    "Ex3.3.\tConcatenate Data2 and Data3 by rows.\n",
    "\n",
    "Ex3.4.\tUse join, and change the parameter of how to right, outer and inner.\n",
    "\n",
    "Ex3.5.\tObserve and discuss the differences among the concatenate results and the join results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a.4 Summarizing/Aggregating\n",
    "\n",
    "Summary and aggregation function in Pandas are useful in producing meaningful results. \\\n",
    "Here are some examples:\n",
    "1.\tTo see the minimum and maximum rating per user and group according to age group \\\n",
    "(based on the result from function cut in `bins`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data2.groupby('user_id').agg({'rating':['min','max']})\n",
    "Data2.groupby('bins').agg({'rating':['min','max']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Groupby\n",
    "Ex4.1.\tCheck the min and max rating per age group based on the result from qcut (`bin_qcut`).\n",
    "\n",
    "Ex4.2.\tCheck the min and max rating per age group based on the result from k-means (`class_kmeans`).\n",
    "\n",
    "Ex4.3.\tDisplay the average rating of each movie.\n",
    "\n",
    "Ex4.4.\tDisplay the min and max age of user for each movie.\n",
    "\n",
    "Ex4.5.\tCount the number of users for each movie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a.5 Resampling\n",
    "\n",
    "In this section, the air quality data is used. You can reuse the command where we have converted date time into the proper data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\XXX\\AirQualityUCI.csv\", header=[0], sep=\";\")\n",
    "#this portion handle the missing data\n",
    "threshold=data.isnull().sum()/ len(data) \n",
    "#print(threshold)\n",
    "remove_col = threshold[threshold>0.8].index #get the index of each columns and store in remove_col\n",
    "data.drop(remove_col, axis=1, inplace = True)\n",
    "#print(data.columns)\n",
    "threshold = data.isnull().sum(axis=1)/(len(data.columns)-1)\n",
    "#print(threshold)\n",
    "remove_row = threshold[threshold>0.8].index\n",
    "data.drop(remove_row, axis=0, inplace = True)\n",
    "print(data.head()) #check to make sure it moves only those with missing values >80%\n",
    "print(len(data))\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Time'] = pd.to_datetime(data['Time'], format = '%H.%M.%S').dt.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features of this data are as below:\n",
    "\n",
    "0. Date (DD/MM/YYYY) \n",
    "1. Time (HH.MM.SS) \n",
    "2. True hourly averaged concentration CO in mg/m^3 (reference analyzer) \n",
    "3. PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted) \n",
    "4. True hourly averaged overall Non Metanic HydroCarbons concentration in microg/m^3 (reference analyzer) \n",
    "5. True hourly averaged Benzene concentration in microg/m^3 (reference analyzer) \n",
    "6. PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted) \n",
    "7. True hourly averaged NOx concentration in ppb (reference analyzer) \n",
    "8. PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted) \n",
    "9. True hourly averaged NO2 concentration in microg/m^3 (reference analyzer) \n",
    "10. PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted) \n",
    "11. PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted) \n",
    "12. Temperature in Â°C \n",
    "13. Relative Humidity (%) \n",
    "14. AH Absolute Humidity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tSet the date as index and display the first few rows of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data.set_index(pd.DatetimeIndex(data['Date'])).drop(\"Date\", axis=1)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tResample the data by producing average value by week. This is called downsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.resample(\"W\").mean().ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tProduce the min and max value by week. Note that there are some values are `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.resample(\"W\").agg(['min','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tUpsampling: Produce the average value by 30 minutes and forward fill the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.resample(\"30min\").mean().ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b: Feature Selection\n",
    "\n",
    "In this section, use the data from https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set\n",
    "\n",
    "Here is the information about the data:\\\n",
    "The market historical data set of real estate valuation are collected from Sindian Dist., New Taipei City, Taiwan. The real estate valuation is a regression problem. The data set was randomly split into the training data set (2/3 samples) and the testing data set (1/3 samples).\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "The inputs are as follows:\\\n",
    "X1=the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.) \\\n",
    "X2=the house age (unit: year) \\\n",
    "X3=the distance to the nearest MRT station (unit: meter) \\\n",
    "X4=the number of convenience stores in the living circle on foot (integer) \\\n",
    "X5=the geographic coordinate, latitude. (unit: degree) \\\n",
    "X6=the geographic coordinate, longitude. (unit: degree) \\\n",
    "The output is as follow: \\\n",
    "Y= house price of unit area (10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b.1: Correlation and heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tImport all the modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.random.seed(123)\n",
    "data = pd.read_excel('Real estate valuation data set.xlsx')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tDrop the first columns which contains only numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:,1:8]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tProduce correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = data.corr()\n",
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tProduce the heat map. \\\n",
    "Based on 3 and 4,\\\n",
    "a.\tIdentify the positive and negative correlated variables. \\\n",
    "b.\tIdentify variables that have very low correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cor, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tThe dependent variable is Y, thus we only interest to know which variables are meaningful to determine Y. Let us set the correlation threshold as 0.5. When the features are having correlation +-0.5 and above, then the features are selected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "s = abs(cor['Y house price of unit area'])\n",
    "selectedFeature = s[s>0.5]\n",
    "print(selectedFeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b.2 Variance Inflation Factor (VIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tImport VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tDeclare x (independent variables) and y (dependent variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,0:6] #0 to 5, need to set as 0:6\n",
    "y = data.iloc[:, 6]\n",
    "print(x.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tRun VIF directly and observe the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif=[variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tThen, try to run multiple regression and generate the VIF. Now, observe the result.\\\n",
    "<u>Which features should be selected?</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run regression and generate VIF\n",
    "Reg = sm.OLS(y, x).fit() #OLS = ordinary linear squares regression model\n",
    "pd.DataFrame({'variables': x.columns[0:6], 'VIF':[variance_inflation_factor(x.values, i) for i in range(len(x.columns[0:6]))]})\n",
    "\n",
    "'''Feature 1, 2, 3, 4 and 6. 5 considers too high compare to the other features. '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b.3 Wrapper Method – Forward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tImport mlxtend library. \\\n",
    "(mlxtend = machine learning extension, this library contains a lot of interesting tools for machine learning tasks and data analysis.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tImport modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tBuild linear regression model to use in feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LinearRegression()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tBuild step forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_forward = sfs(LR, k_features = 4, forward = True, floating = False, scoring = 'r2', verbose = 2, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tPerform step forward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_forward = step_forward.fit(X_train, Y_train)\n",
    "print(list(step_forward.k_feature_idx_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tIn this command, we use random forest to select useful features. Which features are selected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector \n",
    "\n",
    "feature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1), k_features=4, forward=True,verbose=2, scoring='roc_auc', cv=5)\n",
    "features=feature_selector.fit(X_train, Y_train)\n",
    "print(list(features.k_feature_idx_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b.4 Wrapper Method – Backward Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tUse linear regression to perform backward elimination. <u>Which features are selected?</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=list(x.columns) #get the columns\n",
    "while(len(cols)>0):  #when there are columns available in cols\n",
    "    p=[]\n",
    "    x_1=x[cols]\n",
    "    x_1=sm.add_constant(x_1)\n",
    "    model=sm.OLS(y,x_1).fit() \n",
    "    #sm is an object of stat models, and we use OLS froms statmodels library\n",
    "    p=pd.Series(model.pvalues.values[1:], index=cols)\n",
    "    pmax=max(p)\n",
    "    features_with_p_max=p.idxmax()\n",
    "    if(pmax>0.05):  #p is the null hypothesis p value, if greater than 0.05, remove feature\n",
    "        cols.remove(features_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "summary = model.summary()\n",
    "print(summary)\n",
    "selected_features=cols\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b.5 Embedded Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tUse random forest to perform backward elimination. <u>Which features are selected?</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1), k_features=4, forward=False,verbose=2, scoring='accuracy', cv=5)\n",
    "features=feature_selector.fit(np.array(X_train.fillna(0)), Y_train)\n",
    "print(list(features.k_feature_idx_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tUse Lasso Regression method. <u>Which features are selected?</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "reg = LassoCV()\n",
    "reg.fit(x,y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(x,y))\n",
    "coef=pd.Series(reg.coef_, index=x.columns)\n",
    "print(coef)\n",
    "print(\"Lasso picked \" + str(sum(coef != 0))+\"variables and eliminated the other \" + str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "#the plot figure below shows which features are selected\n",
    "imp_coef = coef.sort_values()\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Feature Selection\n",
    "1.\tDownload breast cancer data set from this link: https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "[Hints: you can refer to lab 6 to use a ready data set by impute the missing data with median]\n",
    "2.\tPerform feature selection as below: \n",
    "\n",
    "a.\tapplies a forward selection by using linear regression, set k_features = 4. Record the selected features. \\\n",
    "b.\tapplies a filter method by using correlation. Select features with correlation > 0.5. Record the selected features.\\\n",
    "c.\tapplies an embedded method by using Lasso Regression. Record the selected features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
